import streamlit as st
from langchain.chat_models import init_chat_model
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict
from dotenv import load_dotenv
from huggingface_hub import hf_hub_download
import os, shutil

load_dotenv()

llm = init_chat_model("gpt-4o-mini", model_provider="openai")


embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-small",  
    model_kwargs={'device': 'cpu',  "trust_remote_code": True},
    encode_kwargs={'normalize_embeddings': True}
)



@st.cache_resource(show_spinner="áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ...")
def load_data():
    os.makedirs("faiss_index", exist_ok=True)

    faiss_path = hf_hub_download(
        repo_id="anriKo/georgian-civil-code-dataset",
        filename="index.faiss",   
        repo_type="dataset"
    )
    pkl_path = hf_hub_download(
        repo_id="anriKo/georgian-civil-code-dataset",
        filename="index.pkl",    
        repo_type="dataset"
    )

    shutil.copy(faiss_path, "faiss_index/index.faiss")
    shutil.copy(pkl_path, "faiss_index/index.pkl")

    return FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)



vector_store = load_data()

template = """
áƒ¨áƒ”áƒœ áƒ®áƒáƒ  áƒ¡áƒáƒ¥áƒáƒ áƒ—áƒ•áƒ”áƒšáƒáƒ¡ áƒ¡áƒáƒ›áƒáƒ¥áƒáƒšáƒáƒ¥áƒ áƒ™áƒáƒ“áƒ”áƒ¥áƒ¡áƒ˜áƒ¡ áƒ”áƒ¥áƒ¡áƒáƒ”áƒ áƒ¢áƒ˜ áƒ˜áƒ£áƒ áƒ˜áƒ¡áƒ¢áƒ˜. áƒ¨áƒ”áƒœáƒ˜ áƒáƒ›áƒáƒªáƒáƒœáƒáƒ áƒ£áƒáƒáƒ¡áƒ£áƒ®áƒ áƒ›áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒšáƒ˜áƒ¡ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒáƒ¡ áƒ¥áƒáƒ áƒ—áƒ£áƒš áƒ”áƒœáƒáƒ–áƒ”, áƒ›áƒ®áƒáƒšáƒáƒ“ áƒ›áƒáƒ¬áƒáƒ“áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ¡áƒáƒ¤áƒ£áƒ«áƒ•áƒ”áƒšáƒ–áƒ”, áƒ’áƒ áƒáƒ›áƒáƒ¢áƒ˜áƒ™áƒ£áƒšáƒáƒ“ áƒ¡áƒ¬áƒáƒ áƒ˜ áƒ“áƒ áƒ¤áƒáƒ áƒ›áƒáƒšáƒ£áƒ áƒ˜ áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ áƒ¬áƒ˜áƒœáƒáƒ“áƒáƒ“áƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ—.
áƒ˜áƒœáƒ¡áƒ¢áƒ áƒ£áƒ¥áƒªáƒ˜áƒ”áƒ‘áƒ˜:
1. áƒ’áƒáƒáƒœáƒáƒšáƒ˜áƒ–áƒ” áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ áƒ“áƒ áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜ áƒœáƒáƒ‘áƒ˜áƒ¯-áƒœáƒáƒ‘áƒ˜áƒ¯, áƒ áƒáƒ—áƒ áƒ“áƒáƒáƒ“áƒ’áƒ˜áƒœáƒ áƒ§áƒ•áƒ”áƒšáƒ áƒ áƒ”áƒšáƒ”áƒ•áƒáƒœáƒ¢áƒ£áƒ áƒ˜ áƒ›áƒ£áƒ®áƒšáƒ˜.
2. áƒ—áƒ£ áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒ›áƒ” áƒ›áƒ£áƒ®áƒšáƒ˜áƒ áƒ áƒ”áƒšáƒ”áƒ•áƒáƒœáƒ¢áƒ£áƒ áƒ˜, áƒªáƒáƒš-áƒªáƒáƒšáƒ™áƒ” áƒ¨áƒ”áƒ˜áƒ¢áƒáƒœáƒ” áƒ—áƒ˜áƒ—áƒáƒ”áƒ£áƒšáƒ˜ áƒ›áƒ£áƒ®áƒšáƒ˜áƒ¡ áƒªáƒ˜áƒ¢áƒáƒ¢áƒ áƒ“áƒ áƒ›áƒ˜áƒ£áƒ—áƒ˜áƒ—áƒ” áƒ›áƒ˜áƒ¡áƒ˜ áƒœáƒáƒ›áƒ”áƒ áƒ˜ (áƒ›áƒáƒ’., â€áƒ›áƒ£áƒ®áƒšáƒ˜ 1344â€œ).
3. áƒ§áƒáƒ•áƒ”áƒš áƒªáƒ˜áƒ¢áƒáƒ¢áƒáƒ¡ áƒ—áƒáƒœ áƒáƒ®áƒšáƒ“áƒ”áƒ¡ **áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒáƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ˜áƒ— áƒªáƒ˜áƒ¢áƒáƒ¢áƒ** áƒ™áƒáƒ“áƒ”áƒ¥áƒ¡áƒ˜áƒ“áƒáƒœ, áƒ’áƒáƒ›áƒáƒ§áƒáƒ¤áƒ˜áƒšáƒ˜ **áƒ—áƒáƒ›áƒáƒ›áƒ˜ áƒ¨áƒ áƒ˜áƒ¤áƒ¢áƒ˜áƒ—** áƒ›áƒáƒ áƒ™áƒ“áƒáƒ£áƒœáƒ˜áƒ¡ áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ¨áƒ˜.
4. áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ“áƒáƒ˜áƒ¬áƒ§áƒ” áƒ›áƒáƒ™áƒšáƒ”, áƒ–áƒ£áƒ¡áƒ¢áƒ˜ áƒáƒ®áƒ¡áƒœáƒ˜áƒ—, áƒ áƒáƒ›áƒ”áƒšáƒ˜áƒª áƒáƒ¯áƒáƒ›áƒ”áƒ‘áƒ¡ áƒ§áƒ•áƒ”áƒšáƒ áƒ áƒ”áƒšáƒ”áƒ•áƒáƒœáƒ¢áƒ£áƒ  áƒ›áƒ£áƒ®áƒšáƒ¡.
5. áƒáƒ  áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒ áƒáƒœ áƒ’áƒáƒ›áƒáƒ˜áƒ’áƒáƒœáƒ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ, áƒ áƒáƒ›áƒ”áƒšáƒ˜áƒª áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ›áƒáƒªáƒ”áƒ›áƒ£áƒš áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ¨áƒ˜.
6. áƒ—áƒ£ áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ¨áƒ˜ áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ, áƒ–áƒ£áƒ¡áƒ¢áƒáƒ“ áƒ“áƒáƒ¬áƒ”áƒ áƒ”: â€áƒáƒ› áƒ¨áƒ”áƒ™áƒ˜áƒ—áƒ®áƒ•áƒáƒ–áƒ” áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ¡áƒáƒ›áƒáƒ¥áƒáƒšáƒáƒ¥áƒ áƒ™áƒáƒ“áƒ”áƒ¥áƒ¡áƒ¨áƒ˜ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ.â€œ
áƒ›áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒšáƒ˜áƒ¡ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ:
{question}
áƒ áƒ”áƒšáƒ”áƒ•áƒáƒœáƒ¢áƒ£áƒ áƒ˜ áƒ›áƒ£áƒ®áƒšáƒ”áƒ‘áƒ˜:
{context}
áƒáƒáƒ¡áƒ£áƒ®áƒ˜:
"""


prompt = PromptTemplate(
    template=template, input_variables=["question", "context"]
)

class State(TypedDict):
    question: str
    context: List[Document]
    answer: str

def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}

def generate(state: State):
    docs_content = "\n\n".join(
        f"áƒ›áƒ£áƒ®áƒšáƒ˜ {doc.metadata.get('article', '?')}:\n{doc.page_content}"
        for doc in state["context"]
    )
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {
        "question": state["question"],
        "context": state["context"],
        "answer": response.content
    }


graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()

st.set_page_config(page_title="Georgian Civil Code RAG", page_icon="ğŸ“œ", layout="centered")

st.title("ğŸ“œ áƒ¡áƒáƒ¥áƒáƒ áƒ—áƒ•áƒ”áƒšáƒáƒ¡ áƒ¡áƒáƒ›áƒáƒ¥áƒáƒšáƒáƒ¥áƒ áƒ™áƒáƒ“áƒ”áƒ¥áƒ¡áƒ˜áƒ¡ RAG áƒáƒ’áƒ”áƒœáƒ¢áƒ˜")
st.write("áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ áƒ“áƒ áƒ›áƒ˜áƒ˜áƒ¦áƒ”áƒ— áƒ–áƒ£áƒ¡áƒ¢áƒ˜ áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ™áƒáƒ“áƒ”áƒ¥áƒ¡áƒ˜áƒ“áƒáƒœ.")

question = st.text_area("ğŸ‘‰ áƒ—áƒ¥áƒ•áƒ”áƒœáƒ˜ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ:", height=100)

if st.button("ğŸ” áƒ›áƒáƒ«áƒ”áƒ‘áƒœáƒ”"):
    if question.strip():
        with st.spinner("áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒáƒ«áƒ˜áƒ”áƒ‘áƒ..."):
            state = retrieve({"question": question})
        
        docs_content = "\n\n".join(
            f"áƒ›áƒ£áƒ®áƒšáƒ˜ {doc.metadata.get('article', '?')}:\n{doc.page_content}"
            for doc in state["context"]
        )
        messages = prompt.invoke({"question": question, "context": docs_content})
        
        st.subheader("áƒáƒáƒ¡áƒ£áƒ®áƒ˜:")
        placeholder = st.empty()
        accumulated_response = ""
        
        with st.spinner("áƒáƒáƒ¡áƒ£áƒ®áƒ˜áƒ¡ áƒ’áƒ”áƒœáƒ”áƒ áƒáƒªáƒ˜áƒ..."):
            response_stream = llm.stream(messages)
            first_chunk = next(response_stream)
            accumulated_response += first_chunk.content
            placeholder.markdown(accumulated_response)
        
        for chunk in response_stream:
            accumulated_response += chunk.content
            placeholder.markdown(accumulated_response)
        
    else:
        st.warning("áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒáƒ— áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ.")